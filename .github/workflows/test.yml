name: RDO Map Overlay Tests

on:
  push:
    branches: [ main, initial_implementation, release/* ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'
  MAP_URL: 'https://github.com/${{ github.repository }}/releases/download/assets-v1/rdr2_map_hq.png.gz'

jobs:
  test:
    runs-on: windows-latest
    timeout-minutes: 20

    steps:
    - uses: actions/checkout@v4

    # === AGGRESSIVE CACHING STRATEGY ===

    # Cache 1: Python dependencies
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: |
          ~\AppData\Local\pip\Cache
        key: pip-test-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          pip-test-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

    # Cache 2: Map file (167MB - expensive to download)
    - name: Cache map file
      id: cache-map
      uses: actions/cache@v4
      with:
        path: data/rdr2_map_hq.png
        key: map-file-v1-${{ hashFiles('data/rdr2_map_hq.png') }}
        restore-keys: |
          map-file-v1-

    # Cache 3: Test data and pyramids
    - name: Cache test data
      uses: actions/cache@v4
      with:
        path: |
          data/cache/*.pkl
          tests/data/*.png
          tests/__pycache__
        key: test-data-${{ runner.os }}-${{ hashFiles('tests/*.py') }}
        restore-keys: |
          test-data-${{ runner.os }}-

    # Cache 4: Python virtual environment
    - name: Cache virtual environment
      uses: actions/cache@v4
      with:
        path: venv
        key: venv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          venv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

    # === SETUP ===

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      shell: pwsh
      run: |
        # Use virtual environment if cached
        if (Test-Path "venv") {
          Write-Host "Using cached virtual environment"
          & venv\Scripts\Activate.ps1

          # Verify packages are installed
          python -m pip list > installed.txt 2>&1
          $required = @("opencv-python", "numpy", "flask", "mss", "pillow")
          $missing = $false

          foreach ($pkg in $required) {
            if (-not (Select-String -Path installed.txt -Pattern $pkg -Quiet)) {
              $missing = $true
              break
            }
          }

          if ($missing) {
            Write-Host "Some packages missing, reinstalling..."
            python -m pip install --upgrade pip
            pip install -r requirements.txt
          } else {
            Write-Host "All packages present in cache"
          }
        } else {
          Write-Host "Creating new virtual environment"
          python -m venv venv
          & venv\Scripts\Activate.ps1
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        }

    - name: Pull Git LFS files (map)
      if: steps.cache-map.outputs.cache-hit != 'true'
      shell: pwsh
      run: |
        Write-Host "Map not in cache, pulling from Git LFS..."

        # Install Git LFS if not available
        git lfs install

        # Pull LFS files (map is tracked in Git LFS)
        git lfs pull --include="data/rdr2_map_hq.png"

        # Verify file was pulled
        if (Test-Path "data/rdr2_map_hq.png") {
          $size = (Get-Item "data/rdr2_map_hq.png").Length
          $sizeMB = [math]::Round($size / 1MB, 2)
          Write-Host "Map file pulled from Git LFS: $sizeMB MB"
        } else {
          Write-Host "::error::Failed to pull map file from Git LFS"
          exit 1
        }

    - name: Verify map file
      shell: pwsh
      run: |
        if (Test-Path "data/rdr2_map_hq.png") {
          $size = (Get-Item "data/rdr2_map_hq.png").Length
          $sizeMB = [math]::Round($size / 1MB, 2)

          if ($size -gt 1MB) {
            Write-Host "Map file found: $sizeMB MB"
          } else {
            Write-Host "::warning::Map file exists but appears to be a placeholder ($sizeMB MB)"
          }
        } else {
          Write-Host "::warning::Map file not found. Tests may fail."
        }

    # === RUN TESTS ===

    - name: Run synthetic tests
      id: accuracy
      shell: pwsh
      continue-on-error: true
      run: |
        # Activate virtual environment
        & venv\Scripts\Activate.ps1

        # Run tests
        python tests/test_matching.py > test_results.txt 2>&1
        $exitCode = $LASTEXITCODE

        # Display results
        Get-Content test_results.txt

        # Extract metrics if test ran successfully
        if ($exitCode -eq 0 -or (Test-Path test_results.txt)) {
          $content = Get-Content test_results.txt -Raw

          # Extract success rate
          if ($content -match "Success Rate:.*?(\d+/\d+)") {
            $SUCCESS_RATE = $matches[1]
            echo "success_rate=$SUCCESS_RATE" >> $env:GITHUB_OUTPUT
          } else {
            echo "success_rate=0/0" >> $env:GITHUB_OUTPUT
          }

          if ($content -match "Success Rate:.*?(\d+\.\d+%)") {
            $SUCCESS_PCT = $matches[1]
            echo "success_pct=$SUCCESS_PCT" >> $env:GITHUB_OUTPUT
          } else {
            echo "success_pct=0%" >> $env:GITHUB_OUTPUT
          }

          # Calculate pass/fail (require ≥ 60% success)
          if ($SUCCESS_RATE -match "(\d+)/(\d+)") {
            $passed = [int]$matches[1]
            $total = [int]$matches[2]
            if ($total -gt 0 -and ($passed / $total) -ge 0.60) {
              echo "passed=true" >> $env:GITHUB_OUTPUT
            } else {
              echo "passed=false" >> $env:GITHUB_OUTPUT
            }
          } else {
            echo "passed=false" >> $env:GITHUB_OUTPUT
          }
        } else {
          echo "success_rate=0/0" >> $env:GITHUB_OUTPUT
          echo "success_pct=0%" >> $env:GITHUB_OUTPUT
          echo "passed=false" >> $env:GITHUB_OUTPUT
        }

        exit 0  # Don't fail the workflow here

    - name: Run real gameplay tests
      id: real_tests
      if: always()
      continue-on-error: true
      shell: pwsh
      run: |
        # Activate virtual environment
        & venv\Scripts\Activate.ps1

        # Run tests only if test files exist
        if (Test-Path "tests/run_real_tests.py") {
          python tests/run_real_tests.py > real_test_results.txt 2>&1
          $exitCode = $LASTEXITCODE

          Get-Content real_test_results.txt

          # Extract metrics
          $content = Get-Content real_test_results.txt -Raw

          if ($content -match "Success rate:.*?(\d+/\d+)") {
            $REAL_SUCCESS = $matches[1]
            echo "real_success=$REAL_SUCCESS" >> $env:GITHUB_OUTPUT
          } else {
            echo "real_success=0/0" >> $env:GITHUB_OUTPUT
          }

          if ($content -match "Success rate:.*?(\d+\.\d+%)") {
            $REAL_SUCCESS_PCT = $matches[1]
            echo "real_success_pct=$REAL_SUCCESS_PCT" >> $env:GITHUB_OUTPUT
          } else {
            echo "real_success_pct=0%" >> $env:GITHUB_OUTPUT
          }

          if ($content -match "Average time:.*?(\d+\.?\d*)") {
            $REAL_AVG_TIME = $matches[1]
            echo "real_avg_time=${REAL_AVG_TIME}ms" >> $env:GITHUB_OUTPUT
          } else {
            echo "real_avg_time=0ms" >> $env:GITHUB_OUTPUT
          }

          # Pass if ≥ 80% success
          if ($REAL_SUCCESS -match "(\d+)/(\d+)") {
            $passed = [int]$matches[1]
            $total = [int]$matches[2]
            if ($total -gt 0 -and ($passed / $total) -ge 0.80) {
              echo "real_passed=true" >> $env:GITHUB_OUTPUT
            } else {
              echo "real_passed=false" >> $env:GITHUB_OUTPUT
            }
          } else {
            echo "real_passed=false" >> $env:GITHUB_OUTPUT
          }
        } else {
          Write-Host "::warning::Real gameplay tests not found, skipping"
          echo "real_success=N/A" >> $env:GITHUB_OUTPUT
          echo "real_success_pct=N/A" >> $env:GITHUB_OUTPUT
          echo "real_avg_time=N/A" >> $env:GITHUB_OUTPUT
          echo "real_passed=true" >> $env:GITHUB_OUTPUT
        }

    - name: Check performance regression
      id: performance
      shell: pwsh
      run: |
        # Extract average matching time from synthetic tests
        if (Test-Path test_results.txt) {
          $content = Get-Content test_results.txt -Raw

          if ($content -match "Overall.*?(\d+\.?\d*)ms") {
            $AVG_TIME = [double]$matches[1]
            echo "avg_time=${AVG_TIME}ms" >> $env:GITHUB_OUTPUT

            # Adjusted thresholds for CI environment
            # CI runners are slower, so we increase thresholds
            if ($AVG_TIME -gt 500) {
              Write-Host "::warning::Performance regression detected: ${AVG_TIME}ms average (CI threshold: 500ms)"
              echo "regression=critical" >> $env:GITHUB_OUTPUT
            } elseif ($AVG_TIME -gt 300) {
              Write-Host "::warning::Performance slower than target: ${AVG_TIME}ms average (CI target: <300ms)"
              echo "regression=warning" >> $env:GITHUB_OUTPUT
            } else {
              Write-Host "Performance OK: ${AVG_TIME}ms average"
              echo "regression=none" >> $env:GITHUB_OUTPUT
            }
          } else {
            echo "avg_time=N/A" >> $env:GITHUB_OUTPUT
            echo "regression=unknown" >> $env:GITHUB_OUTPUT
          }
        } else {
          echo "avg_time=N/A" >> $env:GITHUB_OUTPUT
          echo "regression=unknown" >> $env:GITHUB_OUTPUT
        }

    # === REPORTING ===

    - name: Post results summary
      if: always()
      shell: pwsh
      run: |
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "## Test Results"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value ""

        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "### Synthetic Tests"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Accuracy**: ${{ steps.accuracy.outputs.success_rate }} (${{ steps.accuracy.outputs.success_pct }})"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Average Time**: ${{ steps.performance.outputs.avg_time }}"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Status**: ${{ steps.accuracy.outputs.passed == 'true' && '✅ Passed' || '❌ Failed' }}"

        if ("${{ steps.performance.outputs.regression }}" -eq "warning") {
          Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Performance**: ⚠️ Slower than target"
        } elseif ("${{ steps.performance.outputs.regression }}" -eq "critical") {
          Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Performance**: ⚠️ Regression detected (CI environments are slower)"
        } else {
          Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Performance**: ✅ OK"
        }

        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value ""
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "### Real Gameplay Tests"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Accuracy**: ${{ steps.real_tests.outputs.real_success }} (${{ steps.real_tests.outputs.real_success_pct }})"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Average Time**: ${{ steps.real_tests.outputs.real_avg_time }}"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Status**: ${{ steps.real_tests.outputs.real_passed == 'true' && '✅ Passed' || '❌ Failed' }}"

        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value ""
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "### Cache Performance"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Map File**: ${{ steps.cache-map.outputs.cache-hit == 'true' && 'HIT (167MB saved)' || 'MISS' }}"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Test Data**: Cached for faster subsequent runs"
        Add-Content -Path $env:GITHUB_STEP_SUMMARY -Value "- **Python Env**: Virtual environment cached"

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ github.run_number }}
        path: |
          test_results.txt
          real_test_results.txt
        retention-days: 7

    # === CONDITIONAL FAILURE ===

    # Only fail the workflow for severe issues, not performance warnings
    - name: Check for test failures
      if: steps.accuracy.outputs.passed == 'false' && !contains(github.event.head_commit.message, '[skip-tests]')
      shell: pwsh
      run: |
        Write-Host "::error::Synthetic test accuracy too low: ${{ steps.accuracy.outputs.success_rate }} (minimum: 60%)"
        Write-Host ""
        Write-Host "To skip test failures temporarily, include [skip-tests] in commit message"
        exit 1

  # === PERFORMANCE REPORT JOB ===

  performance-report:
    if: always()
    needs: test
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Generate cache report
        run: |
          echo "## Test Caching Report"
          echo ""
          echo "### Caching Strategy:"
          echo "1. **Python packages** - pip cache (~100MB)"
          echo "2. **Map file** - 167MB PNG file"
          echo "3. **Test data** - Generated pyramids and test images"
          echo "4. **Virtual environment** - Complete Python environment"
          echo ""
          echo "### Expected Performance:"
          echo "- **Cold run**: ~10-15 minutes"
          echo "- **Cached run**: ~3-5 minutes"
          echo "- **Map download**: Skipped when cached (saves 2-3 minutes)"
          echo ""
          echo "### Optimization Tips:"
          echo "- Tests use continue-on-error to gather all metrics"
          echo "- Performance thresholds adjusted for CI environments"
          echo "- Virtual environment cached to avoid pip installs"
          echo "- Map file compressed with gzip for faster downloads"